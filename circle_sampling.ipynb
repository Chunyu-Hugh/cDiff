{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "项目路径: /home/chu034/Yaohang_Li/cDiff\n",
            "Python 路径已更新\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "# Project path setup\n",
        "project_root = Path.cwd()\n",
        "if (project_root / 'src').exists():\n",
        "    pass\n",
        "else:\n",
        "    # Fallback: user-specific path\n",
        "    project_root = Path('/home/chu034/Yaohang_Li/cDiff')\n",
        "\n",
        "\n",
        "if not project_root.exists() or not (project_root / 'src').exists():\n",
        "    raise FileNotFoundError(f\"找不到项目目录或 src: {project_root}\")\n",
        "\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "print(f\"项目路径: {project_root}\")\n",
        "print(\"Python 路径已更新\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda:0\n",
            "Loading from: /home/chu034/Yaohang_Li/cDiff/result/circle\n"
          ]
        }
      ],
      "source": [
        "# Config: set your trained checkpoint info here\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datasets import load_dataset\n",
        "from models.neural_sampler import DiffusionPosteriorSampler, NormalizingFlowPosteriorSampler\n",
        "from utils import load_torch_model, SET_SEED\n",
        "\n",
        "# ---- User-editable ----\n",
        "DATASET = \"circle\"            # circle dataset\n",
        "MODEL = \"Diffusion\"           # \"Diffusion\" or \"NormalizingFlow\"\n",
        "USE_ENCODER = True            # circle uses set of (x,y) -> use encoder\n",
        "DATA_TYPE = \"iid\"             # encoder type used in training (iid/time/set)\n",
        "\n",
        "SAVE_ROOT = \"/home/chu034/Yaohang_Li/cDiff/result\"        # root path used during training (see main_circle.py comment)\n",
        "EPOCHS_NAME = \"5000\"          # model_name token, usually epochs (e.g., \"5000\")\n",
        "SEED = 1                      # seed index used during training\n",
        "N_POST_SAMPLES = 1000         # how many posterior samples to draw\n",
        "DIFFUSION_STEPS = 18          # steps for Diffusion sampler\n",
        "DEVICE_INDEX = 0              # GPU index; falls back to CPU if no CUDA\n",
        "N_OBS_POINTS = None           # if None, dataset picks random size per batch\n",
        "BATCH_SIZE = 2                # small batch just to fetch one observation\n",
        "# -----------------------\n",
        "\n",
        "# Compute resolved paths/types\n",
        "DEVICE = torch.device(f\"cuda:{DEVICE_INDEX}\" if torch.cuda.is_available() else \"cpu\")\n",
        "MODEL_TYPE = MODEL\n",
        "SAVE_DIR = os.path.join(SAVE_ROOT, DATASET)\n",
        "\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"Loading from: {SAVE_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "theta_dim: 1, y_dim: 2\n",
            "Encoder is for iid data. If not, please check it.\n",
            "Model loaded from /home/chu034/Yaohang_Li/cDiff/result/circle/5000_seed=1_Diffusion.pth\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DiffusionPosteriorSampler(\n",
              "  (summary): DeepSetSummary(\n",
              "    (instance_embedder): InstanceEmbedder(\n",
              "      (embedder): Sequential(\n",
              "        (0): Linear(in_features=2, out_features=256, bias=True)\n",
              "        (1): SiLU()\n",
              "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (3): SiLU()\n",
              "        (4): Linear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (rho): Sequential(\n",
              "      (0): Linear(in_features=261, out_features=256, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (3): ReLU()\n",
              "      (4): Linear(in_features=256, out_features=256, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (decoder): ScoreNetwork(\n",
              "    (embed): Sequential(\n",
              "      (0): SinusoidalPosEmb()\n",
              "      (1): Linear(in_features=16, out_features=32, bias=True)\n",
              "      (2): Mish()\n",
              "      (3): Linear(in_features=32, out_features=16, bias=True)\n",
              "    )\n",
              "    (layers): MLPNetwork(\n",
              "      (layers): ModuleList(\n",
              "        (0): Linear(in_features=273, out_features=256, bias=True)\n",
              "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (2): Mish()\n",
              "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (4): Mish()\n",
              "        (5): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (6): Mish()\n",
              "        (7): Linear(in_features=256, out_features=1, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (act): Mish()\n",
              "  )\n",
              "  (diffusion): KarrasSDE()\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Build the model by inferring dimensions from a tiny dataloader\n",
        "# Then load the trained weights\n",
        "SET_SEED(42)\n",
        "\n",
        "dataset_generator, _, _ = load_dataset(DATASET)\n",
        "\n",
        "# Create a tiny loader to infer y/theta shapes; for encoder we don't set n_sample\n",
        "if USE_ENCODER:\n",
        "    dl = dataset_generator(n_batches=1, batch_size=BATCH_SIZE, return_ds=False)\n",
        "else:\n",
        "    # If no encoder, dataset expects n_sample=1 to get shape [B, 1, y_dim] collapsed later\n",
        "    dl = dataset_generator(n_batches=1, batch_size=BATCH_SIZE, n_sample=1, return_ds=False)\n",
        "\n",
        "theta_batch, y_batch = next(iter(dl))\n",
        "\n",
        "y_dim = y_batch.shape[-1]\n",
        "theta_dim = theta_batch.shape[1]\n",
        "\n",
        "print(f\"theta_dim: {theta_dim}, y_dim: {y_dim}\")\n",
        "\n",
        "# Construct model skeleton\n",
        "if MODEL == \"Diffusion\":\n",
        "    # circle is simple; use fixed sigma_data=0.5 as in EDM unless you trained differently\n",
        "    model = DiffusionPosteriorSampler(\n",
        "        y_dim=y_dim,\n",
        "        x_dim=theta_dim,\n",
        "        n_summaries=256,\n",
        "        num_hidden_layer=4,\n",
        "        device=DEVICE,\n",
        "        use_encoder=USE_ENCODER,\n",
        "        data_type=DATA_TYPE,\n",
        "        sigma_data=0.5,\n",
        "    )\n",
        "elif MODEL == \"NormalizingFlow\":\n",
        "    model = NormalizingFlowPosteriorSampler(\n",
        "        y_dim=y_dim,\n",
        "        x_dim=theta_dim,\n",
        "        n_summaries=256,\n",
        "        hidden_dim_decoder=32,\n",
        "        n_flows_decoder=32,\n",
        "        alpha=0.1,\n",
        "        device=DEVICE,\n",
        "        use_encoder=USE_ENCODER,\n",
        "        data_type=DATA_TYPE,\n",
        "    ).to(DEVICE)\n",
        "else:\n",
        "    raise NotImplementedError(\"MODEL should be 'Diffusion' or 'NormalizingFlow'\")\n",
        "\n",
        "# Load weights with naming convention used in utils.py\n",
        "# File: {SAVE_DIR}/{EPOCHS_NAME}_seed={SEED}_{MODEL_TYPE}.pth\n",
        "model = load_torch_model(model, SAVE_DIR, EPOCHS_NAME, SEED, MODEL_TYPE)\n",
        "model = model.to(DEVICE)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'int' object has no attribute 'shape'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDIFFUSION_STEPS\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_env/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Yaohang_Li/cDiff/models/neural_sampler.py:116\u001b[39m, in \u001b[36mDiffusionPosteriorSampler.sample\u001b[39m\u001b[34m(self, y, num_steps)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@torch\u001b[39m.no_grad()\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, num_steps=\u001b[32m18\u001b[39m):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_encoder \u001b[38;5;28;01melse\u001b[39;00m y\n\u001b[32m    117\u001b[39m         \u001b[38;5;66;03m# z, log_p,_ = self.diffusion.sample(self.decoder,s,num_steps)\u001b[39;00m\n\u001b[32m    118\u001b[39m         z = \u001b[38;5;28mself\u001b[39m.diffusion.edm_sampler(\u001b[38;5;28mself\u001b[39m.decoder, s, num_steps=num_steps)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Yaohang_Li/cDiff/models/summary.py:49\u001b[39m, in \u001b[36mDeepSetSummary.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     batch_size, n_samples, n_features = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;66;03m# Apply z-score normalization\u001b[39;00m\n\u001b[32m     52\u001b[39m     normalized_x, (mean, std_dev) = zscore(x, dim=\u001b[32m1\u001b[39m)\n",
            "\u001b[31mAttributeError\u001b[39m: 'int' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "model.sample(2, num_steps=DIFFUSION_STEPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-0.2147, -1.8893],\n",
              "         [-1.5477, -1.1046],\n",
              "         [ 1.0587,  1.5794],\n",
              "         ...,\n",
              "         [ 0.2263, -1.8879],\n",
              "         [ 1.7881, -0.6467],\n",
              "         [ 1.1142, -1.5408]],\n",
              "\n",
              "        [[-0.4710,  0.1513],\n",
              "         [ 0.3405,  0.3588],\n",
              "         [ 0.4742, -0.1408],\n",
              "         ...,\n",
              "         [-0.1419,  0.4739],\n",
              "         [ 0.0509,  0.4921],\n",
              "         [ 0.4775,  0.1293]]], device='cuda:0')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 3, got 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Sample posterior draws\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m MODEL == \u001b[33m\"\u001b[39m\u001b[33mDiffusion\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     theta_samples = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_cond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDIFFUSION_STEPS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     26\u001b[39m     theta_samples = model.sample(y_cond)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_env/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Yaohang_Li/cDiff/models/neural_sampler.py:116\u001b[39m, in \u001b[36mDiffusionPosteriorSampler.sample\u001b[39m\u001b[34m(self, y, num_steps)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@torch\u001b[39m.no_grad()\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, num_steps=\u001b[32m18\u001b[39m):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_encoder \u001b[38;5;28;01melse\u001b[39;00m y\n\u001b[32m    117\u001b[39m         \u001b[38;5;66;03m# z, log_p,_ = self.diffusion.sample(self.decoder,s,num_steps)\u001b[39;00m\n\u001b[32m    118\u001b[39m         z = \u001b[38;5;28mself\u001b[39m.diffusion.edm_sampler(\u001b[38;5;28mself\u001b[39m.decoder, s, num_steps=num_steps)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Yaohang_Li/cDiff/models/summary.py:49\u001b[39m, in \u001b[36mDeepSetSummary.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     batch_size, n_samples, n_features = x.shape\n\u001b[32m     51\u001b[39m     \u001b[38;5;66;03m# Apply z-score normalization\u001b[39;00m\n\u001b[32m     52\u001b[39m     normalized_x, (mean, std_dev) = zscore(x, dim=\u001b[32m1\u001b[39m)\n",
            "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ],
      "source": [
        "# Prepare one observed y and draw posterior samples for z\n",
        "import torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def prepare_observation(y_batch, use_encoder: bool, L: int):\n",
        "    \"\"\"\n",
        "    Prepare conditioning tensor for model.sample.\n",
        "    - With encoder: expect y as [B, n_points, y_dim]; take first and repeat to [L, n_points, y_dim].\n",
        "    - Without encoder: expect y as [B, y_dim]; take first and repeat to [L, y_dim].\n",
        "    \"\"\"\n",
        "    y0 = y_batch[0]\n",
        "    if y0.ndim == 1:\n",
        "        y0 = y0.unsqueeze(0)  # [y_dim] -> [1, y_dim]\n",
        "    return y0.repeat(L, *([1] * (y0.ndim - 1)))\n",
        "\n",
        "# Move data to device\n",
        "y_batch = y_batch.to(DEVICE)\n",
        "\n",
        "# Build conditioning for a single observed dataset\n",
        "y_cond = prepare_observation(y_batch, use_encoder=USE_ENCODER, L=N_POST_SAMPLES)\n",
        "\n",
        "# Sample posterior draws\n",
        "if MODEL == \"Diffusion\":\n",
        "    theta_samples = model.sample(y_cond, num_steps=DIFFUSION_STEPS)\n",
        "else:\n",
        "    theta_samples = model.sample(y_cond)\n",
        "\n",
        "theta_samples = theta_samples.detach().cpu().numpy()  # shape [L, 1] for circle\n",
        "\n",
        "print(f\"theta_samples shape: {theta_samples.shape}\")\n",
        "print(f\"mean={theta_samples.mean():.4f}, std={theta_samples.std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot: posterior histogram of z and the observed (x, y) points\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Observed points (take the first set)\n",
        "y_obs = y_batch[0].detach().cpu().numpy()  # shape [n_points, 2] for circle\n",
        "\n",
        "# A crude reference radius from observed points (mean radius)\n",
        "r_est = np.sqrt((y_obs[:, 0] ** 2 + y_obs[:, 1] ** 2)).mean()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Left: histogram of posterior z\n",
        "axes[0].hist(theta_samples.squeeze(), bins=40, alpha=0.8, color='steelblue', edgecolor='k')\n",
        "axes[0].axvline(r_est, color='red', linestyle='--', label=f\"r_est≈{r_est:.3f}\")\n",
        "axes[0].set_title(\"Posterior samples of z (radius)\")\n",
        "axes[0].set_xlabel(\"z\")\n",
        "axes[0].set_ylabel(\"Frequency\")\n",
        "axes[0].legend()\n",
        "\n",
        "# Right: scatter of observed (x, y)\n",
        "axes[1].scatter(y_obs[:, 0], y_obs[:, 1], s=8, alpha=0.7)\n",
        "axes[1].set_aspect('equal', 'box')\n",
        "axes[1].set_title(\"Observed (x, y) points\")\n",
        "axes[1].set_xlabel(\"x\")\n",
        "axes[1].set_ylabel(\"y\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: save samples and the figure to disk\n",
        "import pandas as pd\n",
        "\n",
        "OUT_DIR = \"./samples_circle\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "base = f\"{DATASET}_{MODEL_TYPE}_seed{SEED}_L{N_POST_SAMPLES}\"\n",
        "np.save(os.path.join(OUT_DIR, base + \".npy\"), theta_samples)\n",
        "pd.DataFrame(theta_samples, columns=[\"z\"]).to_csv(os.path.join(OUT_DIR, base + \".csv\"), index=False)\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(os.path.join(OUT_DIR, base + \".npy\"))\n",
        "print(os.path.join(OUT_DIR, base + \".csv\"))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
