{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e5705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook-ready minimal training for circle dataset\n",
    "# CHANGED: converted script to runnable notebook; dataset set to circle\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from models.neural_sampler import NormalizingFlowPosteriorSampler, DiffusionPosteriorSampler\n",
    "from utils import *\n",
    "\n",
    "# ===== Config (edit here) =====\n",
    "DATASET = \"circle\"  # CHANGED: use circle dataset\n",
    "MODEL = \"Diffusion\"  # \"NormalizingFlow\" or \"Diffusion\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 200\n",
    "N_BATCHES = 2\n",
    "BATCH_SIZE = 512\n",
    "LR = 1e-3\n",
    "LR_DECAY = False\n",
    "ALPHA = 0.1\n",
    "USE_ENCODER = False\n",
    "NUM_HIDDEN_LAYER = 4\n",
    "SAVE_DIR = \"./test\"\n",
    "\n",
    "# ===== Load circle dataset =====\n",
    "# CHANGED: load circle via registry instead of CLI args\n",
    "dataset_generator, sample_theta, sample_data = load_dataset(DATASET)\n",
    "if USE_ENCODER:\n",
    "    dl = dataset_generator(N_BATCHES, BATCH_SIZE, return_ds=False)\n",
    "else:\n",
    "    dl = dataset_generator(N_BATCHES, BATCH_SIZE, n_sample=1, return_ds=False)\n",
    "\n",
    "theta, y = next(iter(dl))\n",
    "y_dim = y.shape[-1]\n",
    "theta_dim = theta.shape[1]\n",
    "\n",
    "# ===== Build model =====\n",
    "if MODEL == \"NormalizingFlow\":\n",
    "    model = NormalizingFlowPosteriorSampler(\n",
    "        y_dim=y_dim,\n",
    "        x_dim=theta_dim,\n",
    "        n_summaries=256,\n",
    "        hidden_dim_decoder=32,\n",
    "        n_flows_decoder=32,\n",
    "        alpha=ALPHA,\n",
    "        device=DEVICE,\n",
    "        use_encoder=USE_ENCODER,\n",
    "        data_type=\"iid\",\n",
    "    ).to(DEVICE)\n",
    "elif MODEL == \"Diffusion\":\n",
    "    sigma_data = 0.5  # keep simple for notebook\n",
    "    model = DiffusionPosteriorSampler(\n",
    "        y_dim=y_dim,\n",
    "        x_dim=theta_dim,\n",
    "        n_summaries=256,\n",
    "        num_hidden_layer=NUM_HIDDEN_LAYER,\n",
    "        device=DEVICE,\n",
    "        use_encoder=USE_ENCODER,\n",
    "        data_type=\"iid\",\n",
    "        sigma_data=sigma_data,\n",
    "    )\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "# ===== Optimizer/Scheduler =====\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "\n",
    "# ===== Recreate DL with dataset object for reset =====\n",
    "# CHANGED: request ds so we can call reset_batch_sample_sizes per epoch\n",
    "dl, ds = dataset_generator(N_BATCHES, BATCH_SIZE, None if USE_ENCODER else 1, return_ds=True)\n",
    "\n",
    "# ===== Training Loop =====\n",
    "loss_record = []\n",
    "training_time_record = []\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    epoch_loss = []\n",
    "\n",
    "    for batch in dl:\n",
    "        theta, y = batch\n",
    "        if y.shape[1] == 1:\n",
    "            y = y.squeeze(1)\n",
    "        y = y.to(DEVICE)\n",
    "        theta = theta.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(x=theta, y=y).mean()\n",
    "        epoch_loss.append(float(loss))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    if LR_DECAY:\n",
    "        scheduler.step()\n",
    "\n",
    "    # CHANGED: reset sample sizes per epoch for streaming circle data\n",
    "    ds.reset_batch_sample_sizes()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}  Loss {np.mean(epoch_loss):.4f}  LR {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    loss_record.append(np.mean(epoch_loss))\n",
    "    training_time_record.append(time.time() - start_time)\n",
    "\n",
    "# ===== Save loss (optional) =====\n",
    "os.makedirs(f\"{SAVE_DIR}/{DATASET}\", exist_ok=True)\n",
    "df_loss = pd.DataFrame({\n",
    "    'epochs': list(range(1, len(loss_record)+1)),\n",
    "    'loss': loss_record,\n",
    "})\n",
    "safe_update(df_loss, f\"{SAVE_DIR}/{DATASET}/loss.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244d556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from models.neural_sampler import NormalizingFlowPosteriorSampler, DiffusionPosteriorSampler\n",
    "from evaluation.SBC import sample_sbc_calstats, evaluate_sbc\n",
    "from evaluation.TARP import get_ecp_area_difference\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "def trainer(data_loader, dataset, model, optimizer, scheduler, epochs, device, lr_decay, n_cal, L, seed, model_type, eval_interval, save_path):\n",
    "    evaluation_sbc = pd.DataFrame()\n",
    "    loss_record = []\n",
    "    training_time_record = []\n",
    "    evaluation_ecp = pd.DataFrame(columns=['epoch', 'inference_time', 'ecp_score'])\n",
    "    ecp_traj = pd.DataFrame()\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        epoch_loss = []\n",
    "\n",
    "        for batch in data_loader:\n",
    "            theta, y = batch\n",
    "            if y.shape[1] == 1:\n",
    "                y = y.squeeze(1)\n",
    "            y = y.to(device)\n",
    "            theta = theta.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(x=theta, y=y).mean()\n",
    "            epoch_loss.append(float(loss))\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        if lr_decay:\n",
    "            scheduler.step()\n",
    "\n",
    "        dataset.reset_batch_sample_sizes()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch + 1}/{epochs},\",\n",
    "            f\"Loss: {np.mean(epoch_loss):.2f},\",\n",
    "            f\"LR: {scheduler.get_last_lr()[0]:.4f}\"\n",
    "        )\n",
    "\n",
    "        loss_record.append(np.mean(epoch_loss))\n",
    "        training_time_record.append(time.time() - start_time)\n",
    "\n",
    "        if epoch % eval_interval == 0:\n",
    "            inference_start_time = time.time()\n",
    "\n",
    "            # sbc_calstats = sample_sbc_calstats(dataset, n_cal, L, theta.shape[-1], model, device)\n",
    "            # eval_df = evaluate_sbc(sbc_calstats, seed, epoch, model_type)\n",
    "            # evaluation_sbc = pd.concat([evaluation_sbc, eval_df], ignore_index=True)\n",
    "            #\n",
    "            # inference_time = time.time() - inference_start_time\n",
    "            #\n",
    "            # # Compute ecp score and ecp trajectory\n",
    "            # ecp_score, ecp, alpha = get_ecp_area_difference(dataset, model, device, n_sim=args.ecp_n_sim, n_samples=args.ecp_n_samples)\n",
    "            #\n",
    "            # # Save metrics in the new metrics_df DataFrame\n",
    "            # evaluation_ecp = pd.concat([evaluation_ecp, pd.DataFrame({\n",
    "            #     'epochs': [epoch],\n",
    "            #     'inference_time': [inference_time],\n",
    "            #     'ecp_score': [ecp_score],\n",
    "            #     'seed': [seed],\n",
    "            #     'model_type': [model_type]\n",
    "            # })], ignore_index=True)\n",
    "            #\n",
    "            # # Record ecp trajectory\n",
    "            # ecp_traj[f\"{model_type}_epoch_{epoch}_seed_{seed}\"] = ecp\n",
    "            # ecp_traj.index = alpha\n",
    "\n",
    "            save_model(model, save_path, epoch, seed, model_type)\n",
    "\n",
    "    epochs = list(range(1, len(loss_record) + 1))\n",
    "    df_loss = pd.DataFrame({\n",
    "        'epochs': epochs,\n",
    "        'loss': loss_record,\n",
    "        'seed': seed,\n",
    "        'model_type': model_type,\n",
    "        'training_time': training_time_record\n",
    "    })\n",
    "\n",
    "    return model, evaluation_sbc, evaluation_ecp, df_loss, ecp_traj\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # Dataset paramaters\n",
    "    n_batches = args.n_batches\n",
    "    batch_size = args.batch_size\n",
    "\n",
    "    # Model paramaters\n",
    "    hidden_dim_summary_net = 32\n",
    "    n_summaries = 256  # sufficient statistics for normal-gamma model\n",
    "    DEVICE = args.device\n",
    "    alpha = args.alpha\n",
    "    use_encoder = bool(args.use_encoder)\n",
    "    n_sample = None if use_encoder else 1\n",
    "\n",
    "    # Opitimzer paramaters\n",
    "    epochs = args.epochs\n",
    "    lr = args.lr\n",
    "    lr_decay = args.lr_decay\n",
    "\n",
    "    # Evaluate paramaters\n",
    "    n_cal, L, model_type = args.n_cal, args.L, args.model\n",
    "\n",
    "    if args.nickname is not None:\n",
    "        model_type += args.nickname\n",
    "\n",
    "    n_run = args.n_run\n",
    "    eval_interval = args.eval_interval\n",
    "\n",
    "    dataset_generator, sample_theta, sample_data = load_dataset(args.dataset)\n",
    "\n",
    "    if args.use_encoder:\n",
    "        dl = dataset_generator(n_batches, batch_size, return_ds=False)\n",
    "    else:\n",
    "        dl = dataset_generator(n_batches, batch_size, n_sample=1, return_ds=False)\n",
    "    theta, y = next(iter(dl))\n",
    "    y_dim = y.shape[-1]\n",
    "    theta_dim = theta.shape[1]\n",
    "\n",
    "    for i in range(1,n_run+1):\n",
    "        seed = i + args.seed_start\n",
    "        SET_SEED(seed)\n",
    "\n",
    "        if args.model == \"NormalizingFlow\":\n",
    "            model = NormalizingFlowPosteriorSampler(y_dim=y_dim, x_dim=theta_dim, n_summaries=n_summaries,\n",
    "                                       hidden_dim_decoder=hidden_dim_summary_net, n_flows_decoder=32, alpha=alpha, device=DEVICE,\n",
    "                                       use_encoder=use_encoder, data_type=args.data_type).to(DEVICE)\n",
    "        elif args.model == \"Diffusion\":\n",
    "            if args.use_emperical_sigma:\n",
    "                sigma_data = theta.std().item()\n",
    "            else:\n",
    "                sigma_data = 0.5\n",
    "            num_hidden_layer = args.num_hidden_layer\n",
    "            model = DiffusionPosteriorSampler(y_dim=y_dim, x_dim=theta_dim, n_summaries=n_summaries,num_hidden_layer=num_hidden_layer,\n",
    "                                              device=DEVICE,use_encoder=use_encoder, data_type=args.data_type, sigma_data=sigma_data)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "        save_path = f\"{args.save_path}/{args.dataset}\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        dl, ds = dataset_generator(n_batches, batch_size, n_sample, return_ds=True)\n",
    "\n",
    "        if args.load_model:\n",
    "            model = load_torch_model(model, save_path, epochs, seed, model_type)\n",
    "        else:\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            optimizer_sched = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "\n",
    "            # Training\n",
    "            model, evaluation_sbc, evaluation_ecp, df_loss, ecp_traj = trainer(dl,ds,model,optimizer,optimizer_sched,epochs,DEVICE,lr_decay,n_cal, L, seed,\n",
    "                                         model_type, eval_interval, save_path)\n",
    "\n",
    "        ## Final Evaluation\n",
    "        sbc_calstats = sample_sbc_calstats(ds, n_cal, L, theta_dim, model, DEVICE)\n",
    "        eval_df = evaluate_sbc(sbc_calstats, seed, epochs, model_type)\n",
    "        evaluation_sbc = pd.concat([evaluation_sbc, eval_df], ignore_index=True)\n",
    "        \n",
    "        evaluation_sbc_save_path = f\"{save_path}/evaluation_sbc.csv\"\n",
    "        evaluation_ecp_save_path = f\"{save_path}/evaluation_ecp.csv\"\n",
    "        df_loss_save_path = f\"{save_path}/loss.csv\"\n",
    "        ecp_traj_save_path = f\"{save_path}/ecp_traj.csv\"\n",
    "        safe_update(evaluation_sbc, evaluation_sbc_save_path)\n",
    "        safe_update(evaluation_ecp, evaluation_ecp_save_path)\n",
    "        safe_update(df_loss, df_loss_save_path)\n",
    "        safe_update(ecp_traj, ecp_traj_save_path, axis=1)\n",
    "\n",
    "        if args.save_model:\n",
    "            save_model(model, save_path, epochs, seed, model_type)\n",
    "\n",
    "        # plot_hist(sbc_calstats,save_path,seed,model_type)\n",
    "        if args.dataset in [\"socks\", \"species_sampling\",\"dirichlet_laplace\"]:\n",
    "            plot_scatter(y.squeeze(1),theta,model,save_path,seed,model_type,DEVICE)\n",
    "\n",
    "        if args.dataset == \"cos\":\n",
    "            from datasets.cos import plot_posterior, sample_and_plot\n",
    "            plot_posterior(y_observed = 0.5)\n",
    "            sample_and_plot(0.5,model,save_path, DEVICE, model_type, sample_steps=100, seed=seed)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"A simple example of argparse\")\n",
    "\n",
    "    ## Training parameters\n",
    "    parser.add_argument('--epochs', type=int, default=5000)\n",
    "    parser.add_argument('--model', type=str, default=\"NormalizingFlow\", help=\"NormalizingFlow or Diffusion\")\n",
    "    parser.add_argument('--n_run', type=int, default=1, help=\"How many runs to repeat\")\n",
    "    parser.add_argument('--lr', type=float, default=1e-3)\n",
    "    parser.add_argument('--lr_decay', action='store_true',)\n",
    "    parser.add_argument('--n_batches', type=int, default=2, help=\"Number of batches for an epoch\")\n",
    "    parser.add_argument('--batch_size', type=int, default=512)\n",
    "    parser.add_argument('--save_path', type=str, default=\"./test\")\n",
    "    parser.add_argument('--alpha', type=float, default=0.1, help=\"Parameter for normalizing flow to control Lipschitz constant.\")\n",
    "    parser.add_argument('--device', type=int, default=3)\n",
    "    parser.add_argument('--use_encoder', action='store_true', help=\"Use summary network or not\")\n",
    "    parser.add_argument('--use_emperical_sigma', action='store_true', help=\"whether to set \\sigma_data as empirical std of data, otherwise 0.5 as EDM\")\n",
    "    parser.add_argument('--num_hidden_layer',type=int, default=4, help=\"Number of hidden layers for diffusion model\")\n",
    "\n",
    "    ## Dataset parameters\n",
    "    parser.add_argument('--dataset', type=str, default=\"dirichlet_multinomial\", help=\"Please see all datasets name in datasets/__init__.py\")\n",
    "    parser.add_argument('--data_type', type=str, default=\"iid\", help=\"iid or time\")\n",
    "\n",
    "    ## Evaluation parameters\n",
    "    parser.add_argument('--n_cal', type=int, default=1000, help=\"Number of calibration for SBC\")\n",
    "    parser.add_argument('--L', type=int, default=100, help=\"Number of posterior samples per x for SBC, same notation with SBC paper\")\n",
    "    parser.add_argument('--ecp_n_sim', type=int, default=1000, help=\"Number of simulations for TARP\")\n",
    "    parser.add_argument('--ecp_n_samples', type=int, default=2000, help=\"Number of posterior samples per x for TARP\")\n",
    "    \n",
    "    # Utility parameters\n",
    "    parser.add_argument('--save_model', action='store_true', help=\"Use encoder or not\")\n",
    "    parser.add_argument('--load_model', action='store_true', help=\"Use encoder or not\")\n",
    "    parser.add_argument('--eval_interval', type=int, default=2)\n",
    "    parser.add_argument('--nickname', type=str, default=None, help=\"Add a nickname to the save folder\")\n",
    "    parser.add_argument('--seed_start', type=int, default=0)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
